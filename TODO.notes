First, just get physics part of arXiv working.  It's already 625k papers, and other
categories don't work so well because we have poor references for them (so no links
and all being small dots means no structure in the map).

Major areas of work follow.

To do:
    --> option to load JSON
    --> Barnes-Hutt needs tuning at end stage to eliminate artifacts
        --> how long would an N^2 iteration take?
    --> can we somehow pull the really far away papers more towards the centre?

--------------------------------------------------------------------------------
--  Connecting the disconnected pieces
--------------------------------------------------------------------------------

Out of the 625k papers, 540k of them are connected (ie form the big graph).  The
rest form the disconnected pieces, which we need to somehow connect to the big
graph so they get placed in a sensible location.

Of the disconnected pieces, here is how they are distributed:

    size 1 occured 78086 times
    size 2 occured 1921 times
    size 3 occured 447 times
    size 4 occured 162 times
    size 5 occured 71 times
    size 6 occured 45 times
    size 7 occured 21 times
    size 8 occured 11 times
    size 9 occured 7 times
    size 10 occured 6 times
    size 11 occured 1 times
    size 12 occured 2 times
    size 14 occured 3 times
    size 15 occured 2 times
    size 18 occured 3 times
    size 19 occured 1 times
    size 20 occured 1 times
    size 23 occured 1 times
    size 45 occured 1 times

For the single disconnected papers, they are generally ones who's references we
fail to extract.  As long as we can extract at least on correct reference, these
would be fixed.  This is a good start.  Nevertheless, there are always going to
be ones which fail, or have no references at all.

For the size-45 piece, it's on hurricanes and weather, mostly by S.Jewson.

We could try loading all spires papers, but only to compute connectivity.
Spires papers would not take part in the quad-tree anti-gravity force calculation,
just in the spring calculation.  But this will still leave some papers disconnected.

Strategy to connect the disconnected pieces: for each disconnected piece, choose
1 paper to be the root (eg the one with highest number of citations).  Then make
fake links from this paper to the main graph.  The fake links can be:
    --> Main category, or all categories, of this paper, attracted to centre of
        that category in the big graph.
    --> Links made from keywords (of either the single root paper, or all papers
        in the disconnected piece) going to 2 or 3 papers in the main graph that
        also have those keywords.  Note: doesn't work connecting keyword of 1
        paper to average position of other papers with that keyword, because then
        everything just gravitates to the centre of the graph.
    --> Links made from authors.

We have now pretty much finished this, using keywords to make fake links.

--------------------------------------------------------------------------------
--  Speeding up and improving nbody code
--------------------------------------------------------------------------------

--> Add an option that does further quad descent on the last 100 or so
    iterations to remove any artificial "lines" due to quad tree division.

--> Improve the contact force between papers so they don't overlap at all.
    --> this is almost finished

--> When you zoom right in, there is a lot of blank area in between papers.
    This is a general "feature" of the graph layout algorithm and the fact
    we are using circles.  It's difficult to fix...

--------------------------------------------------------------------------------
--  Develop the daily pipeline to generate the tiles and labels
--------------------------------------------------------------------------------

The daily pipeline must add new papers for the day and regenerate the graph.
It would use existing locations for papers from previous days.

Pipeline for map generation should be split up as:

--> C nbody does the hard work and outputs JSON [{id,x,y,r}].

--> Program to generate keywords for papers, outputs to a DB table.  This is
    actually orthogonal to maps, and can be used in, eg, kea.

--> Program to work out regions and assign them names, at multiple levels.
    Outputs JSON [{x,y,name}], or possible region boundary.

--> Program to generate tiles.  Probably best to save as png because there is
    so much information that it has similar size to jpg (for high enough quality).

    This program can't be in python because it needs to load a lot of stuff into
    memory.  Some numbers: python took almost 1gig to load the 625k papers, whilst
    go took about 250meg.  Problem with go is that it doesn't have a good
    graphics output package (would like to use cairo), and it's also a bit slow
    loading lots of data from mysql.

--------------------------------------------------------------------------------
--  Tile generation and style
--------------------------------------------------------------------------------

--> Work out a nicer way to render the background (ie instead of 2*radius halo
    for each paper).

    Idea: for each point in the canvas, work out its main arXiv category by
    looking for the majority of papers within a certain radius of that point.
    Use this majority-category to colour the point accordingly.

    Maybe just keep it clean and draw the paper with no BG.  Then have
    "country borders" between categories, and between sub-categories, and
    between keyword areas.

--> When zoomed right in, instead of sending tiles, could send just location,
    radius and category, and the front end can render it on the fly.
    --> maybe not, depends on how complicated the rendering procedure is.

--> Need to use different colours for the different sub-categories of astro-ph,
    and cond-mat, physics, etc.  Need to come up with lots of different colours!

--------------------------------------------------------------------------------
--  Keywords and area labels
--------------------------------------------------------------------------------

The program keywords/getkeywords.py extracts keywords for each paper, and puts
them in a DB table, mapskw.  It is useful also for other things, so is not in
the mapgen/ directory.

We need to label areas of the graph.  Labelling should have level-of-detail, so
the more you zoom in, the more specific the labels become.

For the outer most label, probably just arXiv category is enough.

For the next level, probably can do it by hand, since it won't change often.
For example, pick papers with large citations and assign them an appropriate
label.

For high detail, needs to be automatic, so would use generated keywords.

Would be nice to delineate regions (like country borders) associated with the
same keywords, but this might be difficult to do nicely.

================================================================================
= ATTIC
================================================================================

Try 3D for map generation.  More dimensions makes a better layout.

    ==> Doesn't work, because the graph is dense and difficult to navigate in 3d.
        Also, with 100k+ papers, not possibly to render real time in browser.

    Old notes follow.

    Either project 3D to 2D map, or make an interactive 3D map.

    [project to 2D]
    --> Find orientation that minimses total overlap and project.
    --> Generate on a sphere and then fold sphere out.

    [interactive 3D]
    --> Generate on a sphere and view sphere.  Makes it much simpler to control
        view location and zoom level, since there is no orientation to deal with.
    --> When zoomed in, can send vector data and render 3D in the canvas with full
        orientation control.  But then need to have a sky box background which must
        be precomputed for each position.
    --> For a discrete number of orientations (N,S,E,W,etc) can project view to a
        small number of planes and shift their orientation and size to simulate pan
        and zoom.

Connect the completely disconnected papers using keywords as links, linking to the
average position of all papers with that keyword.
    --> Doesn't seem to work very well, since they all cluster around the
        same spot, since average of keyword position, for all keywords, is in the
        centre of the graph.
